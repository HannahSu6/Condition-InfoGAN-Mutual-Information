{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HannahSu6/Condition-InfoGAN-Mutual-Information/blob/main/CGAN_with_Mutual_Information_Maximization_Qin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conditional Generative Adversarial Network (CGAN) with Mutual Information Maximization\n",
        "This notebook demonstrates the implementation of a CGAN that not only generates images conditioned on specific classes but also incorporates mutual information maximization to improve the quality and diversity of the generated images. Mutual information ensures that the generator maintains relevant features from the input conditions, thus improving the stability and performance of the network.\n",
        "\n",
        "This is a torch demostration code for explaining the Conditional GAN Network integrating mutual information on the MNIST dataset."
      ],
      "metadata": {
        "id": "ehxdfy9sKENb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-_EvU4YJ5gI",
        "outputId": "4e9fe977-5f41-4639-9df9-b12aae0a1b4a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\25421\\anaconda3\\envs\\pyhton39\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import itertools\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Directory Setup for Output Images\n",
        "Directories are set up to store images generated during the training process. This setup helps in visually inspecting the performance of the generator at various stages of training.\n"
      ],
      "metadata": {
        "id": "OO0lQFKxKINj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8ZklekEJ5gJ"
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"images/static/\", exist_ok=True)\n",
        "os.makedirs(\"images/varying_c1/\", exist_ok=True)\n",
        "os.makedirs(\"images/varying_c2/\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simulation of Command Line Arguments\n",
        "For the ease of running this notebook interactively, command line arguments are simulated within the notebook. These arguments help in configuring major parameters such as the number of training epochs, batch size, learning rate, etc., typically passed when executing a script from a command line.\n"
      ],
      "metadata": {
        "id": "61sNHEoBKK2L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-icTJK0J5gK",
        "outputId": "5a76fe31-94ba-4206-8cfa-6bdb6b9fdc99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(n_epochs=20, batch_size=64, lr=0.0002, b1=0.5, b2=0.999, n_cpu=8, latent_dim=62, code_dim=2, n_classes=10, img_size=32, channels=1, sample_interval=400)\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import sys\n",
        "\n",
        "# Simulate command line arguments\n",
        "sys.argv = ['ipykernel_launcher.py', '--n_epochs', '20', '--batch_size', '64']\n",
        "\n",
        "# Create parser and add arguments\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--n_epochs\", type=int, default=100, help=\"number of epochs of training\")\n",
        "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\n",
        "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
        "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
        "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
        "parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n",
        "parser.add_argument(\"--latent_dim\", type=int, default=62, help=\"dimensionality of the latent space\")\n",
        "parser.add_argument(\"--code_dim\", type=int, default=2, help=\"latent code\")\n",
        "parser.add_argument(\"--n_classes\", type=int, default=10, help=\"number of classes for dataset\")\n",
        "parser.add_argument(\"--img_size\", type=int, default=32, help=\"size of each image dimension\")\n",
        "parser.add_argument(\"--channels\", type=int, default=1, help=\"number of image channels\")\n",
        "parser.add_argument(\"--sample_interval\", type=int, default=400, help=\"interval between image sampling\")\n",
        "\n",
        "# Parse arguments\n",
        "opt = parser.parse_args()\n",
        "\n",
        "# Print the parsed options\n",
        "print(opt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weight Initialization\n",
        "Proper weight initialization can significantly affect the training dynamics and stability of GANs. Here, weights are initialized from a normal distribution, which is a common practice for GANs.\n"
      ],
      "metadata": {
        "id": "JxM0gGKpKPOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cuda = True if torch.cuda.is_available() else False\n",
        "\n",
        "\n",
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find(\"BatchNorm\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def to_categorical(y, num_columns):\n",
        "    \"\"\"Returns one-hot encoded Variable\"\"\"\n",
        "    y_cat = np.zeros((y.shape[0], num_columns))\n",
        "    y_cat[range(y.shape[0]), y] = 1.0\n",
        "\n",
        "    return Variable(FloatTensor(y_cat))\n"
      ],
      "metadata": {
        "id": "8rXkqzNRKS8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network Architectures\n",
        "### Generator Architecture\n",
        "The generator's role is to synthesize plausible images from random noise conditioned on class labels and additional latent codes aimed at capturing more complex variations.\n",
        "\n",
        "In the `Generator` class of the CGAN architecture, the input consisting of a noise vector, class labels, and latent codes is first transformed by a linear layer into a high-dimensional feature map. This feature map is reshaped into a three-dimensional volume with dimensions `(batch size, 128, init_size, init_size)`, where `init_size` is a quarter of the target image size (`opt.img_size // 4`). The reshaped output then passes through convolutional blocks that include upsampling steps to double the feature map dimensions twice and convolutional layers to refine features. These steps culminate in a final output with dimensions `(batch size, opt.channels, opt.img_size, opt.img_size)`, where `opt.channels` indicates the number of image color channels. This process efficiently structures the generator to transform the input dimensions into a detailed and appropriately sized image output.\n"
      ],
      "metadata": {
        "id": "V_Sr8p2rKT1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        input_dim = opt.latent_dim + opt.n_classes + opt.code_dim\n",
        "        # noise_vector + one-hot encoding vector + 2\n",
        "\n",
        "        self.init_size = opt.img_size // 4  # Initial size before upsampling\n",
        "        self.l1 = nn.Sequential(nn.Linear(input_dim, 128 * self.init_size ** 2))\n",
        "\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, noise, labels, code):\n",
        "        gen_input = torch.cat((noise, labels, code), -1)\n",
        "        out = self.l1(gen_input)\n",
        "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
        "        img = self.conv_blocks(out)\n",
        "        return img\n",
        "\n"
      ],
      "metadata": {
        "id": "wIrZtf2yKa56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Discriminator Architecture\n",
        "The discriminator's job is to distinguish between real and generated images, providing feedback to the generator. It also helps in inferring the latent codes from the images, which is crucial for mutual information maximization.\n",
        "\n",
        "The `Discriminator` class in the CGAN setup evaluates whether images are real or fake and identifies hidden details through layers that gradually reduce image size. It begins with the image's original channels and uses layers with increasing filters (16, 32, 64, 128) to decrease the image size by half each time. Each layer adds complexity using LeakyReLU for non-linear processing and dropout to prevent overfitting. After these layers, the image data is flattened, combined with label information, and refined through more linear layers. The final output includes a single value that tells if the image is real or fake and a set of values representing hidden codes. This allows the discriminator not only to check image authenticity but also to extract and use hidden data, improving the overall CGAN performance."
      ],
      "metadata": {
        "id": "rrr1P9pQKcA9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDgcEvI0J5gK"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, bn=True):\n",
        "            \"\"\"Returns layers of each discriminator block\"\"\"\n",
        "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
        "            if bn:\n",
        "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
        "            return block\n",
        "\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            *discriminator_block(opt.channels, 16, bn=False),\n",
        "            *discriminator_block(16, 32),\n",
        "            *discriminator_block(32, 64),\n",
        "            *discriminator_block(64, 128),\n",
        "        )\n",
        "\n",
        "        # The height and width of downsampled image\n",
        "        ds_size = opt.img_size // 2 ** 4\n",
        "\n",
        "        self.process = nn.Sequential(\n",
        "            nn.Linear(128 * ds_size ** 2+10, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 48),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            )\n",
        "\n",
        "\n",
        "        # Output layers\n",
        "        self.adv_layer = nn.Sequential(nn.Linear(48, 1))\n",
        "        self.latent_layer = nn.Sequential(nn.Linear(48, opt.code_dim))\n",
        "\n",
        "    def forward(self, img,label):\n",
        "        # print(img.shape)\n",
        "        out = self.conv_blocks(img)\n",
        "        # print(out.shape)\n",
        "        out = out.view(out.shape[0], -1)\n",
        "        out = torch.cat([label,out],1)\n",
        "        out = self.process(out)\n",
        "        # print(out.shape)\n",
        "        validity = self.adv_layer(out)\n",
        "        latent_code = self.latent_layer(out)\n",
        "\n",
        "        return validity, latent_code"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss functions\n",
        "adversarial_loss = torch.nn.MSELoss()\n",
        "continuous_loss = torch.nn.MSELoss()\n",
        "\n",
        "# Loss weights\n",
        "lambda_con = 0.1\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "if cuda:\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "    adversarial_loss.cuda()\n",
        "    continuous_loss.cuda()\n",
        "\n",
        "# Initialize weights\n",
        "generator.apply(weights_init_normal)\n",
        "discriminator.apply(weights_init_normal)\n"
      ],
      "metadata": {
        "id": "_95edODqKxHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "The MNIST dataset is loaded and transformed to fit the model requirements, such as resizing images and normalizing them. A DataLoader is then configured to automatically batch and shuffle the data for efficient training.\n"
      ],
      "metadata": {
        "id": "OQzRbErfKkhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Configure data loader\n",
        "os.makedirs(\"../../data/mnist\", exist_ok=True)\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST(\n",
        "        \"../../data/mnist\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transforms.Compose(\n",
        "            [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
        "        ),\n",
        "    ),\n",
        "    batch_size=opt.batch_size,\n",
        "    shuffle=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "L7n6e5mOKoKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup of Optimizers\n",
        "Separate optimizers for the generator and discriminator are defined using the Adam optimizer, known for its efficiency in handling sparse gradients and adaptive learning rates.\n"
      ],
      "metadata": {
        "id": "g6vR8xHjK1L6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "optimizer_info = torch.optim.Adam(\n",
        "    itertools.chain(generator.parameters(), discriminator.parameters()), lr=opt.lr, betas=(opt.b1, opt.b2)\n",
        ")\n",
        "\n",
        "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
        "\n",
        "# Static generator inputs for sampling\n",
        "static_z = Variable(FloatTensor(np.zeros((opt.n_classes ** 2, opt.latent_dim))))\n",
        "static_label = to_categorical(\n",
        "    np.array([num for _ in range(opt.n_classes) for num in range(opt.n_classes)]), num_columns=opt.n_classes\n",
        ")\n",
        "static_code = Variable(FloatTensor(np.zeros((opt.n_classes ** 2, opt.code_dim))))\n",
        "\n"
      ],
      "metadata": {
        "id": "0MvE07KVK4PC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function for Sampling Images\n",
        "To monitor the progress of our model, we periodically save generated images. This function generates a fixed grid of images to track how the image quality evolves as training progresses.\n"
      ],
      "metadata": {
        "id": "SSIqpcX8K5zS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def sample_image(n_row, batches_done):\n",
        "    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n",
        "    # Static sample\n",
        "    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, opt.latent_dim))))\n",
        "    static_sample = generator(z, static_label, static_code)\n",
        "    save_image(static_sample.data, \"images/static/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
        "\n",
        "    # Get varied c1 and c2\n",
        "    zeros = np.zeros((n_row ** 2, 1))\n",
        "    c_varied = np.repeat(np.linspace(-1, 1, n_row)[:, np.newaxis], n_row, 0)\n",
        "    c1 = Variable(FloatTensor(np.concatenate((c_varied, zeros), -1)))\n",
        "    c2 = Variable(FloatTensor(np.concatenate((zeros, c_varied), -1)))\n",
        "    sample1 = generator(static_z, static_label, c1)\n",
        "    sample2 = generator(static_z, static_label, c2)\n",
        "    save_image(sample1.data, \"images/varying_c1/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
        "    save_image(sample2.data, \"images/varying_c2/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "ayENiaXBK78S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Process\n",
        "This section outlines the training loop, which alternates between updating the discriminator and the generator. Each pass includes computing loss functions that guide the model weights' updates to improve both the realism of generated images and their alignment with the input conditions.\n"
      ],
      "metadata": {
        "id": "vyxDYfPLK8or"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSr_EiUpJ5gK",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# ----------\n",
        "#  Training\n",
        "# ----------\n",
        "\n",
        "for epoch in range(opt.n_epochs):\n",
        "    for i, (imgs, labels) in enumerate(dataloader):\n",
        "\n",
        "        batch_size = imgs.shape[0]\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)\n",
        "        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)\n",
        "\n",
        "        # Configure input\n",
        "        real_imgs = Variable(imgs.type(FloatTensor))\n",
        "        labels = to_categorical(labels.numpy(), num_columns=opt.n_classes)\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Sample noise and labels as generator input\n",
        "        z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim))))\n",
        "        label_input = to_categorical(np.random.randint(0, opt.n_classes, batch_size), num_columns=opt.n_classes)\n",
        "        code_input = Variable(FloatTensor(np.random.uniform(-1, 1, (batch_size, opt.code_dim))))\n",
        "\n",
        "        # Generate a batch of images\n",
        "        gen_imgs = generator(z, label_input, code_input)\n",
        "\n",
        "        # Loss measures generator's ability to fool the discriminator\n",
        "        validity, _ = discriminator(gen_imgs,label_input)\n",
        "        g_loss = adversarial_loss(validity, valid)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Loss for real images\n",
        "        real_pred, _  = discriminator(real_imgs,labels)\n",
        "        d_real_loss = adversarial_loss(real_pred, valid)\n",
        "\n",
        "        # Loss for fake images\n",
        "        fake_pred, _ = discriminator(gen_imgs.detach(),label_input)\n",
        "        d_fake_loss = adversarial_loss(fake_pred, fake)\n",
        "\n",
        "        # Total discriminator loss\n",
        "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # ------------------\n",
        "        # Information Loss\n",
        "        # ------------------\n",
        "\n",
        "        optimizer_info.zero_grad()\n",
        "\n",
        "        # Sample labels\n",
        "        sampled_labels = np.random.randint(0, opt.n_classes, batch_size)\n",
        "\n",
        "        # Ground truth labels\n",
        "        gt_labels = Variable(LongTensor(sampled_labels), requires_grad=False)\n",
        "\n",
        "        # Sample noise, labels and code as generator input\n",
        "        z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim))))\n",
        "        label_input = to_categorical(sampled_labels, num_columns=opt.n_classes)\n",
        "        code_input = Variable(FloatTensor(np.random.uniform(-1, 1, (batch_size, opt.code_dim))))\n",
        "\n",
        "        gen_imgs = generator(z, label_input, code_input)\n",
        "        _, pred_code = discriminator(gen_imgs, label_input)\n",
        "\n",
        "        info_loss = lambda_con * continuous_loss(pred_code, code_input)\n",
        "\n",
        "        info_loss.backward()\n",
        "        optimizer_info.step()\n",
        "\n",
        "        # --------------\n",
        "        # Log Progress\n",
        "        # --------------\n",
        "\n",
        "        print(\n",
        "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] [info loss: %f]\"\n",
        "            % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item(), info_loss.item())\n",
        "        )\n",
        "        batches_done = epoch * len(dataloader) + i\n",
        "        if batches_done % opt.sample_interval == 0:\n",
        "            sample_image(n_row=10, batches_done=batches_done)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion and Observations\n",
        "At the end of the training, generated images are examined to evaluate the model's performance. Observations regarding the model's ability to generate diverse and realistic images, as well as its stability over training epochs, are discussed. The inclusion of mutual information has been hypothesized to enhance both the diversity and fidelity of the generated samples.\n"
      ],
      "metadata": {
        "id": "4zmXhIR9LFW6"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pyhton39",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}